{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "01f0c5c2-619a-48e0-bb7c-e5e0be009f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "orcid = '0009-0002-2539-9057' # Fill your orcid here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2fe4bc4e-4574-4322-8b18-0c4d33a749fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a8b6cd-4034-4fc4-85a8-e3431dc564f1",
   "metadata": {},
   "source": [
    "We use the `/works` api to list all works related to the orcid. This gives a summary of all works, so citation information is not included. We collect the `put-code` of all works to retrieve the citation information later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3b04331e-4149-4ca3-a0aa-89e3ba892723",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://pub.orcid.org/v3.0/{}/works'.format(orcid),\n",
    "                        headers={\"Accept\": \"application/orcid+json\" })\n",
    "record = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "16f7c42d-623b-421a-8d87-bbb389313e3b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "put_codes = []\n",
    "for work in record['group']:\n",
    "    put_code = work['work-summary'][0]['put-code']\n",
    "    put_codes.append(put_code)\n",
    "put_code = put_codes[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e5d2aa-5233-486e-abce-a0d07a36c5ce",
   "metadata": {},
   "source": [
    "We use the `/<orcid>/work/<put-code>` endpoint to retrieve the citation information for each record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd797a16-0d91-4271-8e1e-b82579a07e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'citation-type': 'bibtex', 'citation-value': '@inproceedings{wang2023alphaexp,\\n  title={Poster: {FuBuKi}: Fuzzing Testing on Bluetooth with Profile Emulation Kit},\\n  author={Chen, Zhao Min and Lin, Yu-Sheng and Lin, Tien-Chih and Yang, Guan-Yan and Wang, Farn},\\n  booktitle={33rd USENIX Security Symposium (USENIX Security 24)},\\n  year={2024}\\n}'}\n",
      "{'citation-type': 'bibtex', 'citation-value': '\\n@Article{icces.2024.013279,\\nAUTHOR = {Ho-Yin Wong, Guan-Yan Yang, Kuo-Hui Yeh, Farn Wang},\\nTITLE = {Application of Simplified Swarm Optimization on Graph Convolutional Networks},\\nJOURNAL = {The International Conference on Computational \\\\& Experimental Engineering and Sciences},\\nVOLUME = {32},\\nYEAR = {2024},\\nNUMBER = {1},\\nPAGES = {1--4},\\nURL = {http://www.techscience.com/icces/v32n1/58870},\\nISSN = {1933-2815},\\nABSTRACT = {<b>1 Introduction</b><br/>\\nThis paper explores various strategies to enhance neural network performance, including adjustments to network architecture, selection of activation functions and optimizers, and regularization techniques. Hyperparameter optimization is a widely recognized approach for improving model performance [2], with methods such as grid search, genetic algorithms, and particle swarm optimization (PSO) [3] previously utilized to identify optimal solutions for neural networks. However, these techniques can be complex and challenging for beginners. Consequently, this research advocates for the use of SSO, a straightforward and effective method initially applied to the LeNet model in 2023 [4]. SSO optimizes various parameters, including kernel number, size, stride, number of units in fully connected layers, and batch size, but has not previously focused on activation functions, optimizers, or regularization. These elements are the focus of the current study. Additionally, as Graph Convolution Neural Networks (GCN) also incorporate convolutional structures, SSO is applied to GCN models as well. The primary contributions of this study are:<br/>\\n1. Demonstrating the applicability of SSO to GCN.<br/>\\n2. Highlighting importance of choosing activation functions and optimizers over structural modifications.<br/>\\n3. Providing a systematic approach to identify effective parameters through SSO.<br/><br/>\\n<b>2 The Proposed Method</b><br/>\\nThe SSO algorithm is characterized by its straightforwardness. The update mechanism for SSO is detailed in Table 1.<br/>\\n<img src=\"http://www.techscience.com/files/icces/image/13279-1.png\" width=\"500px\"><br/>\\nThe update of each variable xi,j within a solution vector Xi = (xi,1, xi,2, …) is determined by a random variable between 0 and 1. Here, gj represents the variable from the best solution, and pi,j denotes the variable from the original solution. The probabilities Cg, Cp and Cw dictate the likelihood that the updated xi,j will match gj, pi,j, xi,j, and a random value, respectively, set at 0.4, 0.7, and 0.9 in this study. The experimental configurations for GCNs are detailed in Tables 2, highlighting the extensive range of variables or hyperparameters involved.<br/>\\n<img src=\"http://www.techscience.com/files/icces/image/13279-2.png\" width=\"500px\"><br/><br/>\\n<b>3 Experiment</b><br/>\\nIn this study, GCNs employed the Cora, Citeseer, and Pubmed datasets. All experiments were conducted using Python 3.10.12 and Pytorch 2.2.1 on Google Colab, executed on an Intel® Xeon® CPU @ 2.00GHz with 16 GB of RAM and an NVIDIA Tesla T4 GPU.<br/>\\nThe application of the Simplified Swarm Optimization (SSO) algorithm involved three steps:<br/>\\n1.Conduct an initial run to identify the optimal activation function and optimizer without constraints.<br/>\\n2.With the activation function and optimizer fixed, determine the optimal network structure and other hyperparameters such as dropout rate or weight decay.<br/>\\n3.Optionally, implement a learning rate scheduler or data augmentation upon finding the optimal solution if deemed necessary.<br/>\\nThe results, indicating the optimal activation function and optimizer, are presented in Figures 1 to 2.<br/>\\n<img src=\"http://www.techscience.com/files/icces/image/13279-3.png\" width=\"500px\"><br/>\\nFollowing steps 2, the SSO algorithm identified an optimal solution with the best activation function (Tanh) and optimizer (Adagrad), as detailed in Tables 3.<br/>\\n<img src=\"http://www.techscience.com/files/icces/image/13279-4.png\" width=\"500px\"><br/><br/>\\n<b>4 Conclusion</b><br/>\\nThis research demonstrates that SSO is an effective method for identifying optimal solutions in GCN. SSO systematically suggests crucial hyperparameters, such as activation functions and optimizers. Additionally, SSO\\'s applicability extends to other neural network architectures. Currently, we are also conducting research to make SSO applicable to more different neural networks. For future work, we think SSO potentially can improve the test case generation [6], test prioritization [7], deep learning process [8], and blockchain latency [9, 10].},\\nDOI = {10.32604/icces.2024.013279}\\n}'}\n",
      "{'citation-type': 'bibtex', 'citation-value': \"@Article{icces.2024.013297,\\nAUTHOR = {Guan-Yan Yang, Yi-Heng Ko, Farn Wang, Kuo-Hui Yeh, Haw-Shiang Chang, Hsueh Yi Chen},\\nTITLE = {Automated Vulnerability Detection Using Deep Learning Technique},\\nJOURNAL = {The International Conference on Computational \\\\& Experimental Engineering and Sciences},\\nVOLUME = {32},\\nYEAR = {2024},\\nNUMBER = {1},\\nPAGES = {1--4},\\nURL = {http://www.techscience.com/icces/v32n1/58871},\\nISSN = {1933-2815},\\nABSTRACT = {<b>1 Introduction</b><br/>\\nEnsuring the absence of exploitable vulnerabilities within applications has always been a critical aspect of software development [1-3]. Traditional code security testing methods often rely on manual inspection or rule-based approaches, which can be time-consuming and prone to human errors. With the recent advancements in natural language processing, deep learning has emerged as a viable approach for code security testing. In this work, we investigated the application of deep learning techniques to code security testing to enhance the efficiency and effectiveness of security analysis in the software development process. In 2022, Wartschinski et al. [1] utilized a Word2Vec model for Python source code embedding, followed by a Long Short-Term Memory (LSTM) [2] model to identify vulnerable code patterns. Building upon their research, we evaluated the performance of two embedding methods in generating code vector representations to improve training efficiency. By leveraging the CodeBERT [3] model instead of Word2Vec, we achieved enhancements in detecting SQL injection vulnerabilities [4]. Additionally, we applied our proposed models to multiple projects collected from GitHub, compared the scan results with existing static testing tools, and evaluated their performance. The results indicate that our approach outperforms commercially available static application security testing (SAST) tools [5-6], showcasing the potential of deep learning in advancing code security testing.<br/><br/>\\n<b>2 Methodology</b><br/>\\nIn this section, we present our approach. Figure.1 is an overview of our method. We collected the vulnerability dataset from Github, and adopted CodeBERT [3] as the embedding method. After transferring the source code into a vector representation, we trained an LSTM model to extract the vulnerable pattern from the code.<br/>\\n<b>2.1 Data Collection and Labeling</b><br/>\\nWe chose SQL injection vulnerability [4] as our detecting objective since it is reported as one of the most common vulnerabilities by OWASP (https://owasp.org/Top10) and CVE databases (https://cve.mitre.org/). On top of that, SQL injection has syntactic features that can be learned by a deep learning model. To collect the SQL injection dataset from GitHub, we employ the PyDriller [7]. We apply keywords to filter the commits we want, such as commit messages containing “SQL injection fixed” or “SQL injection prevented,” etc. We consider the changed part as it potentially would cause SQL injection. Therefore, we mined the commit and marked the changed part as vulnerable.<br/>\\n<b>2.2 Embedding Layers</b><br/>\\nCodeBERT is a bimodal pre-trained model for NL-PL(Natural Language-Programming Language) tasks, which uses a multi-layer bidirectional Transformer as the model architecture, training at a large dataset for six different programming languages and natural language. Different from [1], we use CodeBERT for Python source code embedding and feed all output tokens to an LSTM model.<br/>\\n<b>2.3 LSTM Model Training</b><br/>\\nAfter pre-processing the data, we can start training our LSTM model. To implement the LSTM model, we used the Keras library. There are three layers in the LSTM architecture: LSTM layer, dropout layer, and dense layer. To yield a better performance, the hyper-parameters setting of the model must be carefully chosen. Table.1 shows the hyper-parameters of the LSTM model. Due to space constraints, we will not discuss the details of parameter settings.<br/>\\n<b>2.4 Result</b><br/>\\nTable.2 illustrates the model performance on detecting SQL injection vulnerabilities within source code. One thing that has to be mentioned is that the accuracy here is reported only for completeness reasons. Due to the data imbalance, most of the code will be clean, and vulnerable parts are relatively rare, meaning there are many more negative parts than positive parts. We can see that our model yields a satisfying result on vulnerability detection by reaching 86.2% precision, 80.0% recall, and 83.1% f1-score.<br/><br/>\\n<b>3 Evaluation</b><br/>\\nTo analyze performance, we used the model architectures in [1] and trained them on our dataset. The results are presented in Table.2. Our observation indicates that our model outperforms the research of [1], attributed to CodeBERT's utilization of a self-attention mechanism, facilitating better retrieval of contextual information compared to Word2Vec. Furthermore, CodeBERT is trained with natural language concurrently. In contemporary software development, there is a notable emphasis on writing human-readable source code, involving the use of meaningful names for functions and variables, as well as documenting code in natural language [8]. Therefore, CodeBERT has better results in this scenario.<br/>\\nTo further evaluate our model in real-world scenarios, we curated an additional dataset focusing on SQL injection vulnerabilities and tested it with SAST tools, namely Bandit [5] and Checkmarx [6]. Bandit is an open-source tool for Python source code vulnerability detection, while Checkmarx is a commercial tool offering a comprehensive suite of application security testing, including SAST. Table.3 presents the testing results on 97 Python files potentially containing SQL injection vulnerabilities. Our model exhibited superior performance, attributable to SAST tools often relying on predefined features crafted by humans. However, given the diverse forms of vulnerabilities, human-defined features may not cover all vulnerable modes. In contrast, our method learns from large amounts of data, allowing it to capture semantics and predict vulnerabilities more effectively.<br/><br/>\\n<b>4 Conclusion</b><br/>\\nIn this work, we proposed a method for Python SQL injection vulnerability detection. By reproducing previous work, we found that CodeBERT is more suitable than Word2Vec for source code embedding tasks. After that, we compare our model with two SAST tools in real-world cases, and the result shows that the deep learning model has the potential to win against traditional static analysis tools on vulnerability detection. In addition, our model architecture can further expand to other vulnerabilities and even other languages. Also, we are conducting more experiments to ensure that our method can detect more different types of vulnerabilities.},\\nDOI = {10.32604/icces.2024.013297}\\n}\"}\n",
      "{'citation-type': 'bibtex', 'citation-value': '@article{Yang2024,title = {TPSQLi: Test Prioritization for SQL Injection Vulnerability Detection in Web Applications},journal = {Applied Sciences (Switzerland)},year = {2024},volume = {14},number = {18},author = {Yang, G.-Y. and Wang, F. and Gu, Y.-Z. and Teng, Y.-W. and Yeh, K.-H. and Ho, P.-H. and Wen, W.-L.}}'}\n",
      "{'citation-type': 'bibtex', 'citation-value': '@inproceedings{Lin_2022,\\tdoi = {10.1109/gcce56475.2022.10014031},\\turl = {https://doi.org/10.1109%2Fgcce56475.2022.10014031},\\tyear = 2022,\\tmonth = {oct},\\tpublisher = {{IEEE}},\\tauthor = {Wei-Han Lin and Guan-Yan Yang and Kuo-Hui Yeh},\\ttitle = {Integrating {FIDO} Authentication with New Digital Identity in Taiwan},\\tbooktitle = {2022 {IEEE} 11th Global Conference on Consumer Electronics ({GCCE})}}'}\n",
      "{'citation-type': 'bibtex', 'citation-value': '@article{Yang2022,title = {A Secure Interoperability Management Scheme for Cross-Blockchain Transactions},journal = {Symmetry},year = {2022},volume = {14},number = {12},author = {Yeh, K.-H. and Yang, G.-Y. and Butpheng, C. and Lee, L.-F. and Liu, Y.-H.}}'}\n",
      "{'citation-type': 'bibtex', 'citation-value': '@inproceedings{Yang_2021,\\tdoi = {10.1109/gcce53005.2021.9621818},\\turl = {https://doi.org/10.1109%2Fgcce53005.2021.9621818},\\tyear = 2021,\\tmonth = {oct},\\tpublisher = {{IEEE}},\\tauthor = {Guan-Yan Yang and Kuo-Hui Yeh and Lin-Fa Lee},\\ttitle = {Towards a Novel Interoperability Management Scheme for Cross-blockchain Transactions},\\tbooktitle = {2021 {IEEE} 10th Global Conference on Consumer Electronics ({GCCE})}}'}\n"
     ]
    }
   ],
   "source": [
    "citations = []\n",
    "for put_code in put_codes:\n",
    "    response = requests.get('https://pub.orcid.org/v3.0/{}/work/{}'.format(orcid, put_code),\n",
    "                            headers={\"Accept\": \"application/orcid+json\" })\n",
    "    work = response.json()\n",
    "    print(work['citation'])\n",
    "    if work['citation'] is not None:\n",
    "        citations.append(work['citation']['citation-value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad763df9-261f-41f3-bc32-00921d0a4e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.bib', 'w') as bibfile:\n",
    "    for citation in citations:\n",
    "        bibfile.write(citation)\n",
    "        bibfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65591747",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpsa1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
